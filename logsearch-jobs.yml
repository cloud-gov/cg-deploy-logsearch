instance_groups:
#######################################################
#First deploy group - elasticsearch_master, maintenance
#######################################################
- name: elasticsearch_master
  instances: 3
  jobs:
  - name: bpm
    release: bpm
  - name: elasticsearch
    release: logsearch
    provides:
      elasticsearch: {as: elasticsearch_master}
    consumes:
      elasticsearch: {from: elasticsearch_master}
    properties:
      elasticsearch:
        cluster_name: logsearch
        jvm_options:
          - "-Dlog4j2.formatMsgNoLookups=true"
        node:
          allow_master: true
          allow_data: false
        limits:
          fd: 131072  # 2 ** 17
        health:
          timeout: 900
        recovery:
          delay_allocation_restart: "15m"
        config_options:
          indices.query.bool.max_clause_count: 2048
  - name: snort-config
    release: snort
    properties:
      snort:
        rules:
        - 'alert tcp any any -> any 9200 (msg:"Unexpected logsearch action"; content:"POST"; http_method; content: "logs-app"; http_uri; content:"/_update"; http_uri; classtype:web-application-attack; sid:343080002; rev:1;)'
        - 'alert tcp any any -> any 9200 (msg:"Unexpected logsearch action"; content:"POST"; http_method; content: "logs-platform"; http_uri; content:"/_update"; http_uri; classtype:web-application-attack; sid:343080003; rev:1;)'
        - 'alert tcp any any -> any 9200 (msg:"Unexpected logsearch action"; content:"DELETE"; http_method; content: "logs-app"; http_uri; classtype:web-application-attack; sid:343080004; rev:1;)'
        - 'alert tcp any any -> any 9200 (msg:"Unexpected logsearch action"; content:"DELETE"; http_method; content: "logs-platform"; http_uri; classtype:web-application-attack; sid:343080005; rev:1;)'
        - (( concat "suppress gen_id 1, sig_id 343080004, track by_src, ip " instance_groups.maintenance.networks.services.static_ips.[0] ))
        - 'alert tcp any any -> any 9200 (msg:"Unexpected logsearch action"; content:"DELETE"; http_method; content: "logs-platform"; http_uri; classtype:web-application-attack; sid:343080005; rev:1;)'
        - (( concat "suppress gen_id 1, sig_id 343080005, track by_src, ip " instance_groups.maintenance.networks.services.static_ips.[0] ))
  persistent_disk_type: logsearch_es_master
  stemcell: default
  azs: [z1]
  networks:
  - name: services
    static_ips:
    - (( grab terraform_outputs.logsearch_static_ips.[0]))
    - (( grab terraform_outputs.logsearch_static_ips.[1]))
    - (( grab terraform_outputs.logsearch_static_ips.[2]))
  vm_extensions: [logsearch-lb]
  update:
    max_in_flight: 1 # Should never update more than one ES master node at a time or cluster will go down

- name: redis
  instances: 1
  jobs:
  - name: bpm
    release: bpm
  - {name: redis, release: logsearch-for-cloudfoundry}
  persistent_disk_type: logsearch_redis
  stemcell: default
  azs: [z1]
  networks:
  - name: services

- name: maintenance
  instances: 1
  vm_extensions: [errand-profile]
  jobs:
  - name: bpm
    release: bpm
  - name: elasticsearch
    release: logsearch
    consumes:
      elasticsearch: {from: elasticsearch_master}
    properties:
      elasticsearch:
        jvm_options:
          - "-Dlog4j2.formatMsgNoLookups=true"
  - name: curator
    release: logsearch
    properties:
      curator:
        execute:
          daily: true
          hourly: false
        purge_logs:
          retention_period: 12
          unit: months
        actions:
        - action: forcemerge
          description: >-
            Force merging of older indices to make their shards more compact
            and search-optimized
          options:
            ignore_empty_list: true
            max_num_segments: 1
          filters:
          - filtertype: pattern
            kind: regex
            value: logs*
          - filtertype: age
            source: creation_date
            direction: older
            unit: days
            unit_count: 90
          - filtertype: forcemerged
            max_num_segments: 1
            exclude: true
        - action: index_settings
          description: >-
            Sets indices older than 1 day to be read only
          options:
            index_settings:
              index:
                blocks:
                  write: true
          filters:
          - filtertype: pattern
            kind: regex
            value: logs*
          - filtertype: age
            source: creation_date
            direction: older
            unit: days
            unit_count: 1

  - name: elasticsearch_config
    release: logsearch
    properties:
      elasticsearch_config:
        component_templates:
        - shards-and-replicas: /var/vcap/jobs/elasticsearch_config/index-templates/shards-and-replicas.json
        - index-settings: /var/vcap/jobs/elasticsearch_config/index-templates/index-settings.json
        - index-mappings: /var/vcap/jobs/elasticsearch_config/index-templates/index-mappings.json
        - index-mappings-lfc: /var/vcap/jobs/elasticsearch-config-lfc/component-index-mappings.json
        - index-mappings-app-lfc: /var/vcap/jobs/elasticsearch-config-lfc/component-index-mappings-app.json
        - index-mappings-platform-lfc: /var/vcap/jobs/elasticsearch-config-lfc/component-index-mappings-platform.json
        index_templates:
        - index-mappings-lfc: /var/vcap/jobs/elasticsearch-config-lfc/index-mappings.json
        - index-mappings-app-lfc: /var/vcap/jobs/elasticsearch-config-lfc/index-mappings-app.json
        - index-mappings-platform-lfc: /var/vcap/jobs/elasticsearch-config-lfc/index-mappings-platform.json
  - name: elasticsearch-config-lfc
    release: logsearch-for-cloudfoundry
    properties:
      elasticsearch_config:
        app_index_settings:
          index.mapping.total_fields.limit: 2000
          index.queries.cache.enabled: "false"
        base_index_component_name: index-mappings-lfc
        app_index_component_name: index-mappings-app-lfc
        platform_index_component_name: index-mappings-platform-lfc
        index_mappings_component_name: index-mappings
        index_settings_component_name: index-settings
        shards_and_replicas_component_name: shards-and-replicas
  - name: elasticsearch_exporter
    release: prometheus
    properties:
      elasticsearch_exporter:
        es:
          uri: http://localhost:9200
          all: true

  - name: upload-kibana-objects
    release: logsearch-for-cloudfoundry
    properties:
      cloudfoundry:
        firehose_events:
        - LogMessage
        - ContainerMetric
        system_domain: (( grab $CF_SYSTEM_DOMAIN ))
        user: (( grab $CF_USERNAME ))
        password: (( grab $CF_PASSWORD ))
      kibana_objects:
        upload_patterns:
        - {type: index-pattern, pattern: "/var/vcap/jobs/upload-kibana-objects/kibana-objects/index-pattern/*.json"}
        - {type: search, pattern: "/var/vcap/jobs/upload-kibana-objects/kibana-objects/search/app-*.json"}
        - {type: visualization, pattern: "/var/vcap/jobs/upload-kibana-objects/kibana-objects/visualization/App-*.json"}
        - {type: dashboard, pattern: "/var/vcap/jobs/upload-kibana-objects/kibana-objects/dashboard/App-*.json"}
  # TODO: Drop after https://github.com/cloudfoundry-community/logsearch-for-cloudfoundry/pull/267 is merged
  - name: cron
    release: cron
    properties:
      cron:
        entries:
        - script:
            name: slow-logs
            contents: (( file "cronjobs/slow-logs.sh" ))
          variables:
            HOST: (( grab instance_groups.elasticsearch_master.networks.services.static_ips.[0] ))
          minute: "0"
          hour: "0"
          day: "*"
          month: "*"
          wday: "*"
          user: root
  stemcell: default
  azs: [z1]
  networks:
  - name: services
    static_ips:
    - (( grab terraform_outputs.logsearch_static_ips.[5] ))
  update:
    serial: true # Block on this job to create deploy group 1

#########################################################
#2nd deploy group - elasticsearch_data, kibana, ingestors
#########################################################
- name: elasticsearch_data
  instances: 11
  jobs:
  - name: bpm
    release: bpm
  - name: elasticsearch
    release: logsearch
    consumes:
      elasticsearch: {from: elasticsearch_master}
    properties:
      elasticsearch:
        node:
          allow_master: false
          allow_data: true
        limits:
          fd: 131072  # 2 ** 17
        health:
          timeout: 900
          disable_post_start: true
        recovery:
          delay_allocation_restart: "15m"
        config_options:
          indices.query.bool.max_clause_count: 2048
        jvm_options:
          - "-Dlog4j2.formatMsgNoLookups=true"
  persistent_disk_type: logsearch_es_data
  stemcell: default
  azs: [z1]
  networks:
  - name: services
  update:
    max_in_flight: 1 # Only update 1 ES data node at a time or risk downtime
  env:
    bosh:
      swap_size: 0

- name: kibana
  instances: 2
  jobs:
  - name: bpm
    release: bpm
  - name: elasticsearch
    release: logsearch
    consumes:
      elasticsearch: {from: elasticsearch_master}
    properties:
      elasticsearch:
        heap_size: 2G
        jvm_options:
          - "-Dlog4j2.formatMsgNoLookups=true"
  - name: kibana
    release: logsearch
    provides:
      kibana: {as: kibana_link}
    consumes:
      elasticsearch: {from: elasticsearch_master}
    properties:
      kibana:
        memory_limit: 75
        default_app_id: "dashboard/App-Overview"
        plugins:
        - auth: /var/vcap/packages/kibana-auth-plugin/kibana-auth-plugin.zip
        console.enabled: false
        config_options:
          server.maxPayloadBytes: 4194304
          # max CSV report size ~20M
          # should never exceed http.max_content_length, which defaults to 100M
          xpack.reporting.csv.maxSizeBytes: 20971520
        env:
        - NODE_ENV: production
        source_files: [/var/vcap/jobs/kibana-auth-plugin/config/config.sh]
        health:
          timeout: 600
  - name: kibana-auth-plugin
    release: logsearch-for-cloudfoundry
    properties:
      kibana-auth:
        cloudfoundry:
          skip_ssl_validation: false
          client_id: kibana_oauth2_client
          system_org: cloud-gov-operators # Org Managers of this org get admin access
        session_key: (( param "specify kibana session key" ))
  - name: route_registrar
    release: routing
    properties:
      route_registrar:
        routes:
        - name: logsearch
          registration_interval: 2s
          port: 5601
          health_check:
            name: kibana-up
            script_path: /var/vcap/jobs/kibana/bin/post-start
          timeout: 1s
  - name: bosh-dns
    properties:
      aliases:
        alertmanager-production.service.cf.internal:
        - "*.alertmanager.production-monitoring.prometheus-production.bosh"
        alertmanager-staging.service.cf.internal:
        - "*.alertmanager.staging-monitoring.prometheus-staging.bosh"
        prometheus-production.service.cf.internal:
        - "*.prometheus.production-monitoring.prometheus-production.bosh"
        prometheus-staging.service.cf.internal:
        - "*.prometheus.staging-monitoring.prometheus-staging.bosh"
        prometheus-tooling.service.cf.internal:
        - "*.prometheus-tooling.production-monitoring.prometheus-production.bosh"
      api:
        client:
          tls: "((/dns_api_client_tls))"
        server:
          tls: "((/dns_api_server_tls))"
      cache:
        enabled: true
      health:
        client:
          tls: "((/dns_healthcheck_client_tls))"
        enabled: true
        server:
          tls: "((/dns_healthcheck_server_tls))"
      log_level: WARN
    release: bosh-dns
  stemcell: default
  azs: [z1]
  networks:
  - name: services
  env:
    bosh:
      swap_size: 0

- name: archiver
  jobs:
  - name: bpm
    release: bpm
  - name: elasticsearch
    release: logsearch
    consumes:
      elasticsearch: {from: elasticsearch_master}
    properties:
      elasticsearch:
        jvm_options:
          - "-Dlog4j2.formatMsgNoLookups=true"
        migrate_data_path: true
  - name: archiver_syslog
    release: logsearch
    properties:
      logstash:
        queue:
          max_bytes: 30gb
      logstash_archiver:
        files: 16384
      logstash_ingestor:
        outputs:
        - plugin: s3
          options:
            region: (( grab terraform_outputs.vpc_region ))
            bucket: (( grab terraform_outputs.logsearch_archive_bucket_name ))
            validate_credentials_on_root_bucket: false  # https://github.com/logstash-plugins/logstash-output-s3/issues/132
            server_side_encryption: true
            time_file: 5
            prefix: "%{+yyyy/MM/dd/HH/mm}"
            encoding: "gzip"
  - name: ingestor_cloudfoundry-firehose
    release: logsearch-for-cloudfoundry
    properties:
      cloudfoundry:
        firehose_subscription_id: logsearch-archiver
        firehose_events:
        - LogMessage
        - ContainerMetric
        firehose_client_id: logsearch_firehose_ingestor
        firehose_cc_pull_interval: 300s
        skip_ssl_validation: false
      syslog:
        host: 127.0.0.1
        port: 5514
  persistent_disk_type: logsearch_ingestor
  stemcell: default
  azs: [z1]
  networks:
  - name: services
  vm_extensions: [logsearch-ingestor-profile]

- name: ingestor
  jobs:
  - name: bpm
    release: bpm
  - name: elasticsearch
    release: logsearch
    consumes:
      elasticsearch: {from: elasticsearch_master}
    properties:
      elasticsearch:
        heap_size: 1G
        jvm_options:
          - "-Dlog4j2.formatMsgNoLookups=true"
  - name: ingestor_syslog
    release: logsearch
    consumes:
      elasticsearch: {from: elasticsearch_master}
    provides:
      ingestor: {as: ingestor_link}
    properties:
      logstash:
        jvm_options:
          - "-Dlog4j2.formatMsgNoLookups=true"
        queue:
          max_bytes: 30gb
      logstash_parser:
        elasticsearch:
          # Use per-day indexing strategy
          index: "logs-app-%{+YYYY.MM.dd}"
          index_type: "%{@type}"
          data_hosts: [127.0.0.1]
        filters:
        - logsearch-for-cf: /var/vcap/packages/logsearch-config-logstash-filters/logstash-filters-default.conf
        deployment_dictionary:
        - /var/vcap/packages/logsearch-config/deployment_lookup.yml
        - /var/vcap/jobs/parser-config-lfc/config/deployment_lookup.yml
  - name: ingestor_cloudfoundry-firehose
    release: logsearch-for-cloudfoundry
    properties:
      cloudfoundry:
        firehose_subscription_id: logsearch-ingestor
        firehose_events:
        - LogMessage
        - ContainerMetric
        firehose_client_id: logsearch_firehose_ingestor
        firehose_cc_pull_interval: 300s
        skip_ssl_validation: false
      syslog:
        host: 127.0.0.1
        port: 5514
  - name: parser-config-lfc
    release: logsearch-for-cloudfoundry
  persistent_disk_type: logsearch_ingestor
  stemcell: default
  azs: [z1]
  networks:
  - name: services

###########################
#3nd deploy group - errands
###########################
- name: smoke-tests
  instances: 1
  vm_extensions: [errand-profile]
  stemcell: default
  azs: [z1]
  networks:
  - name: services
  lifecycle: errand
  release: logsearch
  jobs:
  - name: smoke_tests
    release: logsearch
    consumes:
      elasticsearch: {from: elasticsearch_master}
      ingestor_link: {from: ingestor_syslog}
